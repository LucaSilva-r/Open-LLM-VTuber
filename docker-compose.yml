name: OpenLLM-Vtuber

services:
  openllm:
    container_name: openllm_vtuber
    build:
      context: .
    image: silvaluca/openllmvtuber:latest
    volumes:
      - .:/home/vtuber/app
      # - ./conf.yaml:/home/vtuber/app/conf.yaml
      # - ./assets:/home/vtuber/app/assets
      # - ./avatars:/home/vtuber/app/avatars
      # - ./backgrounds:/home/vtuber/app/backgrounds
      # - ./live2d-models:/home/vtuber/app/live2d-models
      # - ./prompts:/home/vtuber/app/prompts
      # - ./models:/home/vtuber/app/models
      # - ./src:/home/vtuber/app/src
      # - ./logs:/home/vtuber/app/logs
      # - ./characters:/home/vtuber/app/characters
      # - ./chat_history:/home/vtuber/app/history
    networks:
      - main
      - api
    environment:
      - TZ=Europe/Rome
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - 12393:12393
    labels:
      traefik.enable: true
      traefik.docker.network: main
      traefik.http.routers.vtuber.rule: Host(`vtuber.silvaserv.it`)
      traefik.http.routers.vtuber.tls: true
      traefik.http.routers.vtuber.tls.certresolver: lets-encrypt
      traefik.http.routers.vtuber.service: vtuber
      traefik.http.routers.vtuber.middlewares: authentik@file
      traefik.http.services.vtuber.loadbalancer.server.port: 12393
    restart: unless-stopped

  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models/llm:/models
    command: >
      --host 0.0.0.0
      --port 8080
      --model /models/Mistral-Nemo-Instruct-2407-Q6_K.gguf
      --ctx-size 16384
      --n-gpu-layers 99
      --parallel 1
      --flash-attn
      --batch-size 512
      --ubatch-size 512
      --cache-type-k f16
      --cache-type-v f16
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  llamacpp-tools:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models/llm:/models
    command: >
      --host 0.0.0.0
      --port 8081
      --model /models/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf
      --ctx-size 8192
      --n-gpu-layers 99
      --parallel 1
      --flash-attn
      --batch-size 512
      --ubatch-size 512
      --cache-type-k f16
      --cache-type-v f16
      --jinja
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "8081:8081"

  llamacpp-intent:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models/llm:/models
    command: >
      --host 0.0.0.0
      --port 8082
      --model /models/Llama-3.2-3B-Instruct-Q8_0.gguf
      --ctx-size 8096
      --parallel 1
      --batch-size 512
      --ubatch-size 512
      --flash-attn
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  main:
    external: true
  api:
